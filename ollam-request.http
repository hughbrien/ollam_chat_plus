###
# curl http://localhost:11434/api/generate -d '{
#  "model": "llama3.2",
#  "prompt": "Why is the sky blue?"
#
POST http://localhost:11434/api/generate

{
  "model": "llama2",
  "prompt": "Write me a simple Java program using LangChain that will interact with Ollama llama3. Also add the ability to parse the responses and capture  ",
  "stream": true
}


###
# curl http://localhost:11434/api/generate -d '{
#  "model": "llama3.2",
#  "prompt": "Why is the sky blue?"
#
POST http://localhost:11434/api/chat

{
  "model": "llama2",
  "prompt": "Write me a simple Java program using LangChain that will interact with Ollama llama3. Also add the ability to parse the responses and capture  ",
  "stream": false
}

###
# curl http://localhost:11434/api/generate -d '{
#  "model": "llama3.2",
#  "prompt": "Why is the sky blue?"
#
POST http://localhost:11434/api/generate

{
  "model": "llama2",
  "prompt": "Write me a simple Java program using LangChain that will interact with Ollama llama3 ",
  "stream": false
}

###
# curl http://localhost:11434/api/generate -d '{
#  "model": "llama3.2",
#  "prompt": "Why is the sky blue?"
#
POST http://localhost:11434/api/generate

{
  "model": "llama3",
  "prompt": "Explain the concept of embeddings in a response from a request  Ollama running llama3",
  "stream": false
}


###
# Embeddings
POST http://localhost:11434/api/embeddings
Content-Type: application/json

{
  "model": "llama3",
  "prompt": "Sample text"
}

###
# curl -X POST http://localhost:11434/api/chat
#  -H "Content-Type: application/json"
#  -d '{
#        "model": "llama3",
#        "messages": [
#           { "role": "user", "content": "Hello, how are you?" }
#        ]
#      }'


POST http://localhost:11434/api/chat
Content-Type: application/json

{
  "model": "llama3",
  "stream": false,
  "messages": [
    {
      "role": "user",
      "content": "Hello, how are you?"
    }
  ]
}

###
POST http://localhost:11434/api/generate
Content-Type: application/json

{
  "model": "llama3",
  "messages": [
    {
      "content": "Hello, how are you?"
    }
  ],
  "stream": true
}


###
#
POST http://localhost:11434/api/generate
Content-Type: application/json

{
  "model": "llama3",
  "prompt": "Explain red/black trees like I'm a junior dev.",
  "options": {
  },
  "stream": true
}



###
#
POST http://localhost:11434/api/generate
Content-Type: application/json

{
  "model": "llama3",
  "prompt": "Explain red/black trees like I'm a junior dev.",
  "options": {
    "temperature": 0.2,
    "top_p": 0.9,
    "num_predict": 256,
    "stop": ["</done>"]
  },
  "stream": true
}


###
POST http://localhost:11434/api/chat
Content-Type: application/json

{
  "model": "llama3:latest",
  "messages": [
    { "role": "system", "content": "You are a blunt performance engineer." },
    { "role": "user", "content": "Why is my latency spiking?" }
  ],
  "stream": true
}

